# VPC Migration

## TOC
1. [Summary](#Summary)
1. [Notes](#Notes)
1. [Prerequisites](#Prerequisites)
1. [Credential setup](#Credentialsetup)
1. [Discovery](#Discovery)
1. [Building Aviatrix infrastructure](#BuildingAviatrixinfrastructure)
1. [Switching the traffic to the Aviatrix transit (Managed TGW)](#SwitchingthetraffictotheAviatrixtransitManagedTGW)
1. [Switching the traffic to the Aviatrix transit (Non-managed TGW)](#SwitchingthetraffictotheAviatrixtransitNon-managedTGW)
1. [Synchronize terraform state with new routetable associations](#Synchronizeterraformstatewithnewroutetableassociations)
1. [Clean up](#Cleanup)
1. [Logs](#Logs)
1. [S3 bucket](#S3bucket)
1. [EIP quota request](#EIPquotarequest)
1. [Configuring IAM](#ConfiguringIAM)
1. [Installing aviatrix-migation on alpine docker image](#Installingaviatrix-migationonalpinedockerimage)
1. [Installing and running aviatrix-migration using python virtual environment](#Installingandrunningaviatrix-migrationusingpythonvirtualenvironment)
1. [Integrating with terraform cloud VCS workflow](#IntegratingwithterraformcloudVCSworkflow)


##  <a id='Summary'></a>Summary

This script discovers VPCs routing info, builds terraform files and allows for switching traffic to the Aviatrix transit

The migration process involves 6 steps:

1. [Preparing the environment](#Prerequisites)

2. [Discovering the VPC(s) configuration](#Discovery)

3. [Building Aviatrix infrastructure](#BuildingAviatrixinfrastructure)

4. [Switching the traffic to the Aviatrix transit](#SwitchingthetraffictotheAviatrixtransitManagedTGW)

5. [Synchronizing terraform state](#Synchronizeterraformstatewithnewroutetableassociations)

6. [Deleting unnecessary resources](#Cleanup)

[Logging](#Logs) and [s3 bucket](#S3bucket) details

##  <a id='Notes'></a>Notes
If using cloudformation, skip 1-4: provide link to cloudformation

##  <a id='Prerequisites'></a>Prerequisites
1. Python 3.7+
2. `pip3 install aviatrix-migration`
3. Ensure Terraform is installed.
4. Ensure correct policy permissions.

   - An AWS role with the permissions to perform the migration.\
   If you are using the default Aviatrix role, it requires the following additions:\
      ***"ec2:ModifyVpcEndpoint",***\
      ***"ec2:DescribeTransitGatewayVpcAttachments",***\
      ***"ec2:GetTransitGatewayAttachmentPropagations",***\
      ***"ec2:ReplaceRouteTableAssociation"***\
      ***"ec2:DisassociateVpcCidrBlock"***\
      ***"ec2:DisassociateSubnetCidrBlock"***\
      ***"ec2:DetachVpnGateway"***\
      ***"ec2:DeleteVpnGateway"***\
      ***"ec2:DeleteNatGateway"***\
      ***"directconnect:DeleteVirtualInterface"***\
      ***"ssm:GetParameter"*** # Optional: Needed when using SSM to store aviatrix controller password.

    - The following permission are required for the script to creating a backup S3 bucket and managing the backup:\
      ***"s3:CreateBucket",***\
      ***"s3:DeleteBucket",***\
      ***"s3:ListBucket",***\
      ***"s3:GetObject",***\
      ***"s3:PutObject",***\
      ***"s3:DeleteObject",***\
      ***"s3:GetBucketVersioning",***\
      ***"s3:PutBucketVersioning",***\
      ***"s3:GetBucketPublicAccessBlock"***\
      ***"s3:PutBucketPublicAccessBlock"***

  A tool is provided to manage the required migration permissions [here](#Configuring-IAM).

##  <a id='Credentialsetup'></a>Credential setup

The python migration script requires both AWS account credential and Aviatrix controller credential for managing cloud resources.
- AWS credential can be provided using environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY or the shared credential file ~/.aws/credentials. In addition, if your migration script is running on a AWS EC2, you can also attach an IAM role with the proper AssumeRole policy.
- Controller credential can be defined using the two environment variables:

  ***export aviatrix_controller_user=admin***\
  ***export aviatrix_controller_password=&lt;password&gt;***

  or it can be passed as a command line argument using the --ctrl_user &lt;username&gt; option, where the password would be prompted separately.

Running terraform also requires the AWS account credential and the Aviatrix controller credential to be setup:
- The AWS account credential is read by terraform using the same approach (environment variables, shared credential file, or instance profile) that is used by the migration script, i.e., the same setup for migration script is reused here. 
- For controller, you configure either the SSM mode or the ENV mode in the tf_controller_access section of the YAML file, so the required terraform resources will be generated by dm.discovery accordingly.
  - In SSM mode, controller password is stored in AWS SSM. You specify in the YAML file the name and location of the SSM store in the AWS controller account, and the AWS provider alias and ssm_role for accessing it.
  - In ENV mode, controller credential is passed to the terraform using the two environment variables AVIATRIX_USERNAME and AVIATRIX_PASSWORD.


##  <a id='Discovery'></a>Discovery
1. Provide [discovery info](README_discovery_input.md) in the discovery.yaml file

2. Run the discovery script to review and resolve the reported alerts \
   ***python3 -m dm.discovery discovery.yaml***

   - The script generates terraform files required to build Aviatrix infrastructure in the migrated VPC. They can be found in **&lt;terraform_output&gt;/&lt;account No&gt;**.

   - The script shows progress by printing to stdout the discovered VPC and detected alerts.  It produces two log files: **dm.log** contains the results of the checks and tasks that are done for each VPC; **dm.alert.log** captures the alert summary that are seen on stdout.  They can be found in **&lt;terraform_output&gt;/log**.

3. Alerts generated by this phase are mostly informational. After resolving any unexpected alerts, run discovery again to regenerate terraform files.

4. [Optional] Upload tf files to S3 with the option --s3backup to backup the **&lt;terraform_output&gt;/&lt;account No&gt;** folder into the S3 bucket **&lt;bucket&gt;/dm/&lt;account No&gt;**: \
   ***python3 -m dm.discovery discovery.yaml --s3backup***

   This will allow the terraform files to be called later in switch_traffic time where new subnet-route-table association will be generated and appended to the existing terraform files.


**Migration restriction.**\
  A single discovery.yaml file can be used to migrate all VPCs within an account in a single migration (discovery/staging/switch_traffic/cleanup) cycle, or multiple yaml files can be used to migrating a group of VPCs at a time, i.e., in seperate migration cycles.  There is one restriction: On the SAME account, a migration cycle must be completed before the next one can be started, i.e., migration cycle cannot be overlapped within the same account.

##  <a id='BuildingAviatrixinfrastructure'></a>Building Aviatrix infrastructure

1. The terraform files generated under the `<terraform_output>/<account_number>` directory contain the Aviatrix infrastructure  resources, the discovered
   subnets and the copied route tables.

   The following steps should be performed in the ***account No*** directory:

2. Run ***terraform init***

3. This step is only required if tf_controller_access.mode in discovery.yaml is set to SSM. \
Run ***terraform apply -target data.aws_ssm_parameter.avx-password*** to create TF state file if none exists.

4. Run ***source ./terraform-import-subnets.sh*** to import discovered subnet resources.

5. Run ***terraform apply*** to build Aviatrix infrastructure . Terraform will deploy and attach the spoke gateways.\
   It will also copy existing route tables.

##  <a id='SwitchingthetraffictotheAviatrixtransitManagedTGW'></a>Switching the traffic to the Aviatrix transit (Managed TGW)
This dm.switch_traffic flow should be used if managed_tgw is set to `True` in YAML file. In this case, each of the VPC's CIDRs will be broken up into more specific CIDRs (advertized by the spoke gateway), so that traffic will flow through the Aviatrix transit and spoke gateways instead of the AWS tgw. The more specific CIDRs are needed to avoid overlapping CIDR ranges (managed by the controller), and to defining the more preferred route.


1. Run the switch_traffic command to switch traffic from AWS TGW/VGW to the Aviatrix transit. \
    This command has two forms, depending whether you want to download the discovery.yaml from S3
(archived during the discovery phase) or use a local input yaml file:

      - Use a local yaml file. \
        ***python3 -m dm.switch_traffic --ctrl_user &lt;controller-admin-id&gt; --yaml_file discovery.yaml***

      - Download discovery.yaml from the S3 bucket to /tmp/discovery.yaml.  The is the typical flow.\
        ***python3 -m dm.switch_traffic --ctrl_user &lt;controller-admin-id&gt; --s3_yaml_download &lt;s3_account&gt;,&lt;s3_account_role&gt;,&lt;s3_bucket&gt;,&lt;spoke_account&gt;***

    **dm.switch_traffic** is responsible for:\
      a) changing the subnets association to the new RTs created in the Building Aviatrix infrastructure phase\
      b) setting up the VPC advertised CIDRs in the Aviatrix spoke gateways.\

    It supports the following additional options:

      - ***--s3backup*** uploads the associated terraform output account folder into S3 at the end of switch_traffic, e.g.:\
      ***python3 -m dm.switch_traffic --s3backup --ctrl_user &lt;controller-admin-id&gt; --s3_yaml_download &lt;s3_account&gt;,&lt;s3_account_role&gt;,&lt;s3_bucket&gt;,&lt;spoke_account&gt;***

      - ***--s3download*** downloads the account folder, containing the terraform script, from S3 at the beginning of switch_traffic. This is required if the local account folder has been removed because of a container restart, e.g.:\
      ***python3 -m dm.switch_traffic --s3download --ctrl_user &lt;controller-admin-id&gt; --s3_yaml_download &lt;s3_account&gt;,&lt;s3_account_role&gt;,&lt;s3_bucket&gt;,&lt;spoke_account&gt;***

      - ***--revert*** restores the configuration back to its original state.\
      It includes restoring the original subnet and route table associations, adding back the deleted VPC-attachment-static routes into TGW routing table, and removing all the spoke gateway advertised CIDRs, e.g.:\
      ***python3 -m dm.switch_traffic -ctrl_user &lt;controller-admin-id&gt; --revert --s3_yaml_download &lt;s3_account&gt;,&lt;s3_account_role&gt;,&lt;s3_bucket&gt;,&lt;spoke_account&gt;***

      - ***--dry_run*** runs through the switch_traffic logic and reports the detected alerts and list of changes to be made **without** actually making any changes, e.g.:\
      ***python3 -m dm.switch_traffic --dry_run --ctrl_user &lt;controller-admin-id&gt; --s3_yaml_download &lt;s3_account&gt;,&lt;s3_account_role&gt;,&lt;s3_bucket&gt;,&lt;spoke_account&gt;***

    If the environment variables for controller username and password are defined (see [Prerequisites](#prerequisites), Step 5), you can run the **switch_traffic**  without the **--ctrl_user** option; in which case, the migration script will not prompt you for the controller password and use the environment variable defined credential instead, e.g.:\
    ***python3 -m dm.switch_traffic --s3_yaml_download &lt;s3_account&gt;,&lt;s3_account_role&gt;,&lt;s3_bucket&gt;,&lt;spoke_account&gt;***


##  <a id='SwitchingthetraffictotheAviatrixtransitNon-managedTGW'></a>Switching the traffic to the Aviatrix transit (Non-managed TGW)
In this case, managed_tgw is set to `False` in YAML file. All VPC CIDRs is advertised by the spoke gateway without any change. dm.switch_traffic will remove the static route and/or propagated route in tgw route table so that traffic will flow through our Avaitrix spoke and transit gateways.


1. Run the switch_traffic command to switch traffic from AWS TGW/VGW to the Aviatrix transit. \
    This command has two forms, depending whether you want to download the discovery.yaml from S3
(archived during the discovery phase) or use a local input yaml file:

      - Use a local yaml file. \
        ***python3 -m dm.switch_traffic --ctrl_user &lt;controller-admin-id&gt; --yaml_file discovery.yaml***

      - Download discovery.yaml from the S3 bucket to /tmp/discovery.yaml.  The is the typical flow.\
        ***python3 -m dm.switch_traffic --ctrl_user &lt;controller-admin-id&gt; --s3_yaml_download &lt;s3_account&gt;,&lt;s3_account_role&gt;,&lt;s3_bucket&gt;,&lt;spoke_account&gt;***

    **dm.switch_traffic** is responsible for:\
      a) changing the subnets association to the new RTs created in the Building Aviatrix infrastructure phase\
      b) deleting the VPC-attachment-static-route in TGW routing table (***--rm_static_route***)\
      c) setting up the VPC advertised CIDRs in the Aviatrix spoke gateways.

    It supports the following additional options:

      - ***--s3backup*** uploads the associated terraform output account folder into S3 at the end of switch_traffic, e.g.:\
      ***python3 -m dm.switch_traffic --s3backup --ctrl_user &lt;controller-admin-id&gt; --rm_static_route --s3_yaml_download &lt;s3_account&gt;,&lt;s3_account_role&gt;,&lt;s3_bucket&gt;,&lt;spoke_account&gt;***

      - ***--s3download*** downloads the account folder, containing the terraform script, from S3 at the beginning of switch_traffic. This is required if the local account folder has been removed because of a container restart, e.g.:\
      ***python3 -m dm.switch_traffic --s3download --ctrl_user &lt;controller-admin-id&gt; --rm_static_route --s3_yaml_download &lt;s3_account&gt;,&lt;s3_account_role&gt;,&lt;s3_bucket&gt;,&lt;spoke_account&gt;***

      - ***--revert*** restores the configuration back to its original state.\
      It includes restoring the original subnet and route table associations, adding back the deleted VPC-attachment-static routes into TGW routing table, and removing all the spoke gateway advertised CIDRs, e.g.:\
      ***python3 -m dm.switch_traffic -ctrl_user &lt;controller-admin-id&gt; --revert --s3_yaml_download &lt;s3_account&gt;,&lt;s3_account_role&gt;,&lt;s3_bucket&gt;,&lt;spoke_account&gt;***

      - ***--dry_run*** runs through the switch_traffic logic and reports the detected alerts and list of changes to be made **without** actually making any changes, e.g.:\
      ***python3 -m dm.switch_traffic --dry_run --ctrl_user &lt;controller-admin-id&gt; --rm_static_route --s3_yaml_download &lt;s3_account&gt;,&lt;s3_account_role&gt;,&lt;s3_bucket&gt;,&lt;spoke_account&gt;***

    If the environment variables for controller username and password are defined (see [Prerequisites](#prerequisites), Step 5), you can run the **switch_traffic**  without the **--ctrl_user** option; in which case, the migration script will not prompt you for the controller password and use the environment variable defined credential instead, e.g.:\
    ***python3 -m dm.switch_traffic --rm_static_route --s3_yaml_download &lt;s3_account&gt;,&lt;s3_account_role&gt;,&lt;s3_bucket&gt;,&lt;spoke_account&gt;***

##  <a id='Synchronizeterraformstatewithnewroutetableassociations'></a>Synchronize terraform state with new routetable associations
(Optional: Only relevant if customer needs to maintain terraform state post migration)\
**switch_traffic** will append new route-table-to-subnet-association resources to the corresponding ***&lt;vpc-id&gt;.tf*** in the ***terraform_output/&lt;account No&gt;*** folder.
This step is needed to sync those back together.

- Import new subnet-association resources into terraform\
   Run ***source ./terraform-import-associations.sh*** at the ***aws_spoke*** folder.


##   <a id='Cleanup'></a>Clean up
1. Run the following comand to delete original route table, VPC-TGW attachments, VPC-TGW attachment subnets, and VPC secondary CIDRs.
When config/filter_tgw_attachment_subnet is True, we will not migrate tgw_attachment_subnet and they will be deleted during dm.cleanup.
This command has two forms, depending whether you want to download the discovery.yaml from S3 (archived at discovery time) or use a local input yaml file:

      - Use a local yaml file. \
        ***python3 -m dm.cleanup --yaml_file discovery.yaml***

      - Download discovery.yaml from S3 to /tmp/discovery.yaml.  The is the typical flow.\
        ***python3 -m dm.cleanup --s3_yaml_download &lt;s3_account&gt;,&lt;s3_account_role&gt;,&lt;s3_bucket&gt;,&lt;spoke_account&gt;***

    **dm.cleanup** is responsible for\
      a) deleting the revert.json from tmp folder locally and from s3.\
      b) deleting the original route tables\
      c) deleting the VPC-attachment\
      d) deleting the subnets attached to the VPC-attachment\
      e) deleting the route tables which are not associated with any subntes\
      f) deleting the secondary cidr's provided in vpc_cidrs\
      g) detaching and deleting the VGW associated with the vpc.

    Cleanup process can be re-ran multiple times.
    It also has a dry_run option:

      ***--dry_run*** allows the users to review the resources to be deleted before the actual clean up, e.g.:

      - Use a local yaml file. \
      ***python3 -m dm.cleanup --dry_run --yaml_file discovery.yaml***

      - Download discovery.yaml from S3 to /tmp/discovery.yaml.  The is the typical flow.\
      ***python3 -m dm.cleanup --dry_run --s3_yaml_download &lt;s3_account&gt;,&lt;s3_account_role&gt;,&lt;s3_bucket&gt;,&lt;spoke_account&gt;***

##   <a id='Logs'></a>Logs

Both **dm.discovery** and **dm.switch_traffic** append its log to the end of the two log files: **dm.log** logs the details resulting from the checks and tasks that are done per VPC. **dm.alert.log** captures the same alert summary that are seen on stdout. Here is an example log that shows the beginning of each **dm.discovery** run:

      2021-05-06 13:58:33,348 #############################################
      2021-05-06 13:58:33,348
      2021-05-06 13:58:33,348 dm.discovery my-managed-regions.yaml
      2021-05-06 13:58:33,348
      2021-05-06 13:58:33,348 #############################################
      2021-05-06 13:58:33,348
      2021-05-06 13:58:33,844 +++++++++++++++++++++++++++++++++++++++++++++
      2021-05-06 13:58:33,844
      2021-05-06 13:58:33,844     Account ID :  123456789012
      2021-05-06 13:58:33,844     Role       :  aviatrix-role-app
      2021-05-06 13:58:33,844
      2021-05-06 13:58:33,844 +++++++++++++++++++++++++++++++++++++++++++++
      2021-05-06 13:58:33,844
      2021-05-06 13:58:33,844 - Check VPC for duplicate name
      2021-05-06 13:58:34,383   **Alert** VPC name test-vpc-test-vpc-test-vpc1 > 26 chars, in us-east-1/vpc-0b111506a32e62fdf
      2021-05-06 13:58:34,395
      2021-05-06 13:58:34,395 =============================================
      2021-05-06 13:58:34,395
      2021-05-06 13:58:34,395     Region     :  us-east-1
      2021-05-06 13:58:34,395
      2021-05-06 13:58:34,395 =============================================
      2021-05-06 13:58:34,395
      2021-05-06 13:58:34,396 - Check EIP usage
      2021-05-06 13:58:35,074   EIP limit:    80
      2021-05-06 13:58:35,075   EIP in use:   2
      2021-05-06 13:58:35,075   EIP required: 6
      2021-05-06 13:58:35,216
      2021-05-06 13:58:35,216 ---------------------------------------------
      2021-05-06 13:58:35,216
      2021-05-06 13:58:35,216     Vpc Name : plain-vpc
      2021-05-06 13:58:35,216     Vpc ID   : vpc-0433454d6edec3b2d
      2021-05-06 13:58:35,216     CIDRs    : ['10.62.0.0/16']
      2021-05-06 13:58:35,216
      2021-05-06 13:58:35,216 ---------------------------------------------
      2021-05-06 13:58:35,216

- The beginning of each **dm.discovery** or **dm.switch_traffic** run is marked by a line of number-sign characters (#), signifying the command and option that were used for the run.  In addition, one can identify the starting point of the latest run in the log by going to the end of the log file and search backward for the number-sign character. Similar structure applies to **dm.alert.log** as well.

- The logs file can be found at <terraform_output>/log.  They are also uploaded to the S3 bucket in <bucket>/dm/<spoke_account>/tmp at
the end of discovery or switch_traffic execution.

##   <a id='S3bucket'></a>S3 bucket

The S3 attributes in YAML specifies the bucket to be used in S3 for storing the logs and the  terraform files of each spoke VPC account.  If a new bucket is specified, **Discovery** will create the bucket with versioning and full privacy enabled.  If this is an existing bucket, **Discovery** will check if the bucket has versioning and full privacy enabled and will alert and terminate immediately if either one of the settings is missing.

- Discovery will backup the content of account folder into S3 at the end of each run when using the flag **--s3backup**.  The account folder contains the terraform files that will be retrieved at staging and switch_traffic time.

- Switch_traffic starts by downloading the account folder so it can
store the new subnet-route-table association resources to the existing terraform file.
At the end, it will upload the latest of the account folder back to S3.

In --revert mode, similar sequence occurs:
1) Terraform files are downloaded for the given accounts.
2) Previously added subnet-route-table resources are removed.
3) Upload all the account files back to S3.

##   <a id='EIPquotarequest'></a>EIP quota request

**dm.mgmt_quota** is a helper script that is used for sending the increase-quota-limit request to AWS. Currently, only EIP is supported.  It has 3 three forms:

- List current limit on EIP:\
***python3 -m dm.mgmt_quota &lt;account_id&gt; &lt;region&gt; &lt;role&gt; eip***

- Request quota increase to the ***&lt;new_limit&gt;***:\
***python3 -m dm.mgmt_quota &lt;account_id&gt; &lt;region&gt; &lt;role&gt; eip --req_quota &lt;new_limit&gt;***\
This command also returns the request ID that can be used for querying the request status (see next example).

- Check request status:\
***python3 -m dm.mgmt_quota &lt;account_id&gt; &lt;region&gt; &lt;role&gt; eip --req_id &lt;req_id&gt;***\
***&lt;req_id&gt;*** is the ID returned by the quota increase request.

Here are the additional IAM permissions required for EIP quota request (L-0263D0A3):
***servicequotas:GetServiceQuota***\
***servicequotas:RequestServiceQuotaIncrease***\
***servicequotas:GetRequestedServiceQuotaChange***

Here is an example in json format:

    {
        "Effect": "Allow",
        "Action": [
            "servicequotas:GetServiceQuota",
            "servicequotas:RequestServiceQuotaIncrease"
        ],
        "Resource": "arn:aws:servicequotas:*:<ACCOUNT_ID>:ec2/L-0263D0A3"
    },
    {
        "Effect": "Allow",
        "Action": [
            "servicequotas:GetRequestedServiceQuotaChange"
        ],
        "Resource": "*"
    },

- First two permissions ***servicequotas:GetServiceQuota*** and ***servicequotas:RequestServiceQuotaIncrease*** are resource-level permission while ***servicequotas:GetRequestedServiceQuotaChange*** is not and requires a wildcard in the Resource value (https://docs.aws.amazon.com/servicequotas/latest/userguide/identity-access-management.html).


##   <a id='ConfiguringIAM'></a>Configuring IAM
`dm.configure_iam` is a helper script to add required migration permissions to the spoke accounts.

To add permissions:
```
$ python -m "dm.configure_iam" add <path_to_discovery_yaml>
```
This will add temporary permission to the Aviatrix app policy required for migration.

To remove temporary permissions:
```
$ python -m "dm.configure_iam" remove <path_to_discovery_yaml>
```

It's possible an account may have exceeded the max amount of policy versions allowed, in this case running both commands with `--force` will remove the oldest policy version from the policy if needed.
```
$ python -m "dm.configure_iam" add <path_to_discovery_yaml> --force
$ python -m "dm.configure_iam" remove <path_to_discovery_yaml> --force
```

Full help text:
```
$ python -m "dm.configure_iam" --help
usage: dm_configure_iam [-h] [--force] {add,remove,rm} ... discovery_config

Update IAM policies

positional arguments:
  {add,remove,rm}
    add             Add required IAM permissions policy.
    remove (rm)     Remove previously added IAM permissions policy.
  discovery_config

options:
  -h, --help        show this help message and exit
  --force, -f
```


##   <a id='Installingaviatrix-migationonalpinedockerimage'></a>Installing aviatrix-migation on alpine docker image
The aviatrix-migation binary has a number of third-party library dependences.  One of which is called **cffi**.  The cffi library is pre-built and available for ubuntu, but not for alpine at the moment. As a result, installion of aviatrix-migration on alpine will trigger a build for the missing cffi library and will fail if the development packages (e.g., gcc) have not been installed. Given below are the steps to install aviatrix-migration on alpine linux.

1. Our reference python-alpine image is taking from https://hub.docker.com/_/python. In particular, we are using python 3.9 and alpine 3.15 for our installation.
2. Create a new folder and change directory into it.
3. Pull the docker file from 3.9-alpine3.15 and name it Dockerfile.  Here is a copy of it:

   ```
   #
   # NOTE: THIS DOCKERFILE IS GENERATED VIA "apply-templates.sh"
   #
   # PLEASE DO NOT EDIT IT DIRECTLY.
   #

   FROM alpine:3.15

   # ensure local python is preferred over distribution python
   ENV PATH /usr/local/bin:$PATH

   # http://bugs.python.org/issue19846
   # > At the moment, setting "LANG=C" on a Linux system *fundamentally breaks Python 3*, and that's not OK.
   ENV LANG C.UTF-8

   # runtime dependencies
   RUN set -eux; \
      apk add --no-cache \
         ca-certificates \
         tzdata \
      ;

   ENV GPG_KEY E3FF2839C048B25C084DEBE9B26995E310250568
   ENV PYTHON_VERSION 3.9.12

   RUN set -eux; \
      \
      apk add --no-cache --virtual .build-deps \
         gnupg \
         tar \
         xz \
         \
         bluez-dev \
         bzip2-dev \
         dpkg-dev dpkg \
         expat-dev \
         findutils \
         gcc \
         gdbm-dev \
         libc-dev \
         libffi-dev \
         libnsl-dev \
         libtirpc-dev \
         linux-headers \
         make \
         ncurses-dev \
         openssl-dev \
         pax-utils \
         readline-dev \
         sqlite-dev \
         tcl-dev \
         tk \
         tk-dev \
         util-linux-dev \
         xz-dev \
         zlib-dev \
      ; \
      \
      wget -O python.tar.xz "https://www.python.org/ftp/python/${PYTHON_VERSION%%[a-z]*}/Python-$PYTHON_VERSION.tar.xz"; \
      wget -O python.tar.xz.asc "https://www.python.org/ftp/python/${PYTHON_VERSION%%[a-z]*}/Python-$PYTHON_VERSION.tar.xz.asc"; \
      GNUPGHOME="$(mktemp -d)"; export GNUPGHOME; \
      gpg --batch --keyserver hkps://keys.openpgp.org --recv-keys "$GPG_KEY"; \
      gpg --batch --verify python.tar.xz.asc python.tar.xz; \
      command -v gpgconf > /dev/null && gpgconf --kill all || :; \
      rm -rf "$GNUPGHOME" python.tar.xz.asc; \
      mkdir -p /usr/src/python; \
      tar --extract --directory /usr/src/python --strip-components=1 --file python.tar.xz; \
      rm python.tar.xz; \
      \
      cd /usr/src/python; \
      gnuArch="$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)"; \
      ./configure \
         --build="$gnuArch" \
         --enable-loadable-sqlite-extensions \
         --enable-optimizations \
         --enable-option-checking=fatal \
         --enable-shared \
         --with-system-expat \
         --with-system-ffi \
         --without-ensurepip \
      ; \
      nproc="$(nproc)"; \
      make -j "$nproc" \
   # set thread stack size to 1MB so we don't segfault before we hit sys.getrecursionlimit()
   # https://github.com/alpinelinux/aports/commit/2026e1259422d4e0cf92391ca2d3844356c649d0
         EXTRA_CFLAGS="-DTHREAD_STACK_SIZE=0x100000" \
         LDFLAGS="-Wl,--strip-all" \
      ; \
      make install; \
      \
      cd /; \
      rm -rf /usr/src/python; \
      \
      find /usr/local -depth \
         \( \
            \( -type d -a \( -name test -o -name tests -o -name idle_test \) \) \
            -o \( -type f -a \( -name '*.pyc' -o -name '*.pyo' -o -name '*.a' \) \) \
         \) -exec rm -rf '{}' + \
      ; \
      \
      find /usr/local -type f -executable -not \( -name '*tkinter*' \) -exec scanelf --needed --nobanner --format '%n#p' '{}' ';' \
         | tr ',' '\n' \
         | sort -u \
         | awk 'system("[ -e /usr/local/lib/" $1 " ]") == 0 { next } { print "so:" $1 }' \
         | xargs -rt apk add --no-network --virtual .python-rundeps \
      ; \
      apk del --no-network .build-deps; \
      \
      python3 --version

   # make some useful symlinks that are expected to exist ("/usr/local/bin/python" and friends)
   RUN set -eux; \
      for src in idle3 pydoc3 python3 python3-config; do \
         dst="$(echo "$src" | tr -d 3)"; \
         [ -s "/usr/local/bin/$src" ]; \
         [ ! -e "/usr/local/bin/$dst" ]; \
         ln -svT "/usr/local/bin/$src" "/usr/local/bin/$dst"; \
      done

   # if this is called "PIP_VERSION", pip explodes with "ValueError: invalid truth value '<VERSION>'"
   ENV PYTHON_PIP_VERSION 22.0.4
   # https://github.com/docker-library/python/issues/365
   ENV PYTHON_SETUPTOOLS_VERSION 58.1.0
   # https://github.com/pypa/get-pip
   ENV PYTHON_GET_PIP_URL https://github.com/pypa/get-pip/raw/38e54e5de07c66e875c11a1ebbdb938854625dd8/public/get-pip.py
   ENV PYTHON_GET_PIP_SHA256 e235c437e5c7d7524fbce3880ca39b917a73dc565e0c813465b7a7a329bb279a

   RUN set -eux; \
      \
      wget -O get-pip.py "$PYTHON_GET_PIP_URL"; \
      echo "$PYTHON_GET_PIP_SHA256 *get-pip.py" | sha256sum -c -; \
      \
      export PYTHONDONTWRITEBYTECODE=1; \
      \
      python get-pip.py \
         --disable-pip-version-check \
         --no-cache-dir \
         --no-compile \
         "pip==$PYTHON_PIP_VERSION" \
         "setuptools==$PYTHON_SETUPTOOLS_VERSION" \
      ; \
      rm -f get-pip.py; \
      \
      pip --version

   CMD ["python3"]
   ```
4. Run the following command to tag and build an image:

   ```
   docker build -t 3-9-alpine-3-15 .
   ```

5. Start the image and drop into the shell /bin/sh.

   ```
   docker run -i -t 3-9-alpine-3-15 /bin/sh
   ```

6. Install the build package so that the missing cffi library can be built locally

   ```
   apk add --no-cache curl python3 pkgconfig python3-dev openssl-dev libffi-dev musl-dev make gcc
   ```

7. Now, run "pip install aviatrix-migration"

## <a id='Installingandrunningaviatrix-migrationusingpythonvirtualenvironment'></a>Installing and running aviatrix-migration using python virtual environment
Python virtual environment (venv) provides the means for isolating conflicting python application. For example, installing a new python application might require an upgrade of a python library that an old python application already depends on, raising concerns for the upgrade.  Here is the step for installing aviatrix-migration manually using python venv with python3.9.

    python3.9 -m venv ./python3-env
    source ./python3-env/bin/activate
    pip3 install discovery-migration

  - Note, python3-env is the folder name to be created by python venv for storing all the virtual environment information.
  - After activation, any python or python3 reference is mapped to the python3.9, that was used to initialize the virtual environment.  In addition, any package installed afterwards are stored and contained inside python3-env folder.
  - To terminate venv, simply type **deactivate**.
  - You can maintain multiple venv at the same time, and delete them if not needed.
  - Automate the installation of aviatrix-migration with venv from a shell script:

    ```
    echo "python3 -m venv ./python3-env"
    python3 -m venv ./python3-env
    echo "source ./python3-env/bin/activate"
    source ./python3-env/bin/activate
    pip3 install discovery-migration
    deactivate
    ```

  - Invoke dm.discovery with venv from a shell script:

    ```
    echo "source ./python3-env/bin/activate"
    source ./python3-env/bin/activate
    python -m dm.discovery dicovery.yaml
    deactivate
    ```

##   <a id='IntegratingwithterraformcloudVCSworkflow'></a>Integrating with terraform cloud VCS workflow

- Setup at terraform cloud

  1. Obtain a terraform cloud account and create an organization

  2. Create a workspace and select VCS flow.

     a. You will need to create a repo at github (or other git cloud) and link it to the workspace (See
     https://www.terraform.io/cloud-docs/vcs#connecting-vcs-providers-to-terraform-cloud for details). This repo will be used to commit the whole content of the **terraform_output** folder, created by dm.discovery. Here is a sample content of the **terraform_output** folder:
     ```
     drwxr-xr-x   5 userabc  staff  160 May  4 21:40 module_aws_brownfield_spoke_vpc
     drwxr-xr-x   4 userabc  staff  128 May  4 21:40 log
     drwxr-xr-x  12 userabc  staff  384 May  5 10:31 212187878444
     ```
     - Note that the numeric folder is referred to as the account folder; its name is the AWS account number of the spoke VPC you are migrating.

     b. While configuring your VCS in terraform cloud, populate the **Terraform Working Directory** field with the name of the **account folder** generated by dm.discovery, so Terraform cloud knows where to run terraform apply.

     c. Configure the **Apply Method** to use **Manual apply**, instead of **Auto apply**. Every commit to the repo will trigger a PLAN-and-APPLY run.  In Manual apply mode, you have a
     chance to review, confirm or discard a run, via the UI.

  3. Define a variable set for storing aws credential and controller credential in  terraform cloud, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AVIATRIX_PASSWORD, and AVIATRIX_USERNAME. You can share this variable set to the workspace just created or to all workspaces.

- Setup at your local terminal

  1. Make sure you are running latest terraform. Pre 1.0.0 does not support terraform **cloud** block.

  2. Export the environment variables AVIATRIX_PASSWORD, and AVIATRIX_USERNAME locally for resolving the aviatrix provider dependence.

  3. In discovery.yaml, comment out tf_controller_access for configuring terraform to read the controller credential via the environment variables.

  4. In discovery.yaml, define **tf_cloud:** under terraform, e.g.:

  ```
    terraform:
      tf_cloud:
        organization: "<your organization name>"
        workspace_name: "<your workspace name>"
  ```

  5. Open up your controller security group to allow terraform cloud access.

  6. Run **terraform login** locally and follow the authentication instruction to copy and store the return token.

- Start migration

  1. Run **dm.discovery** with the discovery.yaml at the terminal
  2. Run **terraform init** at the terminal
  3. Run **source ./terraform-import-subnets.sh** at the terminal.
  4. Commit the terraform_output folder into the VCS repo, which should trigger a PLAN-and-APPLY run at the terraform cloud. Confirm the APPLY to stage the resources.
  6. Run dm.switch_traffic at the terminal
  7. Run **source ./terraform-import-associations.sh** at the terminal.
  8. Commit the terraform_output folder again, and it will trigger another PLAN-and-APPLY run. Confirm the APPLY.
  9. Run **dm.cleanup**.

- Considerations

  1. In cases where docker is used to run discovery migration related commands, the above migration steps should be modified to include repo access for recovering the terraform output folder when needed.
  2. Use one workspace per spoke AWS account to store the terraform state; that is, the workspace will hold the terraform states of all migrated VPCs in this AWS account.
  3. Use multiple workspace for the same AWS account. A separate repo is required for each workspace.
